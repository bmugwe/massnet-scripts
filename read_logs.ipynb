{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import time\n",
    "\n",
    "# Global Variables \n",
    "end_complete=r'\\*1\\*1'\n",
    "end_complete_esc='*1*1'\n",
    "delimiter_000 = r'\\*000\\*'\n",
    "delimiter_000_esc = r'\\*000\\*'\n",
    "delimiter_00 = r'\\*00\\*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "# functions to get offset\n",
    "def getTripplezero(string):\n",
    "    \n",
    "    split_string = string.split(delimiter_000)\n",
    "    curent_string = split_string[len(split_string)-1]\n",
    "\n",
    "    return curent_string\n",
    "\n",
    "def getDoublezero(string):\n",
    "    \n",
    "    split_string = string.split(delimiter_00)\n",
    "    curent_string = split_string[len(split_string)-1]\n",
    "\n",
    "    return curent_string\n",
    "\n",
    "def getTimestamp():\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Access initialisation\n",
    "password = \"123456@Ab\"\n",
    "encode_pass = quote_plus(password)\n",
    "db_url = \"mysql+mysqlconnector://{USER}:{PWD}@{HOST}/{DBNAME}\"\n",
    "source_db_url = db_url.format(USER=\"root\", PWD=encode_pass, HOST=\"localhost:3306\", DBNAME='massnet')\n",
    "source_engine = create_engine(source_db_url, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  filter data to fetch\n",
    "query = \"Select *, length(user_input) as 'string_length',  length(user_input)-length(REPLACE(user_input, '*', '')) AS 'no_of_asterisks' from ussd_audit_logs where is_user = 1 and user_input <> '' and user_input like'4*%' having string_length > 9\"\n",
    "\n",
    "query_11 = \"Select *, length(user_input) as 'string_length',  length(user_input)-length(REPLACE(user_input, '*', '')) AS 'no_of_asterisks' from ussd_audit_logs where is_user = 1 and user_input <> '' and user_input like'%*4*%' having string_length > 9\"\n",
    "\n",
    "incomplete_distribution_query = \"Select *, length(user_input) as 'string_length',  length(user_input)-length(REPLACE(user_input, '*', '')) AS 'no_of_asterisks' from ussd_audit_logs where is_user = 1 and user_input <> '' and (user_input like'%*4*%' or user_input like'4*%') and user_input not like '%*1*1' having string_length >= 9\"\n",
    "\n",
    "# Household registration , unsuccessful and incomplete  \n",
    "HH_REG_UNS = ''\n",
    "HH_REG_INC = ''\n",
    "\n",
    "# Nets distribution , unsuccessful and incomplete\n",
    "NETS_DIST_UNS = ''\n",
    "NETS_DIST_INC = ''\n",
    "\n",
    "\n",
    "query_cond = query.format(Q_CONDITION=HH_REG_INC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "try:\n",
    "    df_distribution = pd.read_sql(query_cond, source_engine)\n",
    "    distribution_details = pd.read_sql('select * from distribution_details', source_engine)\n",
    "    house_holds = pd.read_sql('select * from house_holds_mass_nets', source_engine)\n",
    "except Exception as e:\n",
    "    print(f'Could not read data, encountered the following error:  {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data 2\n",
    "\n",
    "try:\n",
    "    admin_units = pd.read_sql('select * from administrative_units', source_engine)\n",
    "except Exception as e:\n",
    "    print(f'Could not read data, encountered the following error:  {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data 2\n",
    "\n",
    "try:\n",
    "    distr_start_mid_session = pd.read_sql(query_11, source_engine)\n",
    "    incomplete_distribution_query_data = pd.read_sql(incomplete_distribution_query, source_engine)\n",
    "except Exception as e:\n",
    "    print(f'Could not read data, encountered the following error:  {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging distribution details and households to enable filtering out\n",
    "\n",
    "dd_hhmn = distribution_details.merge(house_holds, how='left', left_on='hh_id', right_on='id')\n",
    "# df_distribution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling filtering\n",
    "\n",
    "# result = df_distribution.groupby(['no_of_asterisks', 'string_length'])['id'].count().reset_index(name='COUNT(id)')\n",
    "# filters_result = df_distribution[df_distribution['no_of_asterisks'] == 3 ]\n",
    "\n",
    "passed = df_distribution[df_distribution['user_input'].str.endswith(end_complete)]\n",
    "passed_3_asterisks=passed[passed['no_of_asterisks']==3]\n",
    "\n",
    "\n",
    "incomplete_distribution = df_distribution[~df_distribution['user_input'].str.endswith('1*1')]\n",
    "complete_distribution = df_distribution[df_distribution['user_input'].str.endswith('1*1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling 3 steps\n",
    "def handle3steps(payload):\n",
    "    df_distribution = payload\n",
    "    passed = df_distribution[df_distribution['user_input'].str.endswith(end_complete_esc)]\n",
    "    passed_3_asterisks=passed[passed['no_of_asterisks']==3]\n",
    "    # Explode the column_with_data using the '*' delimiter\n",
    "    df_exploded = passed_3_asterisks['user_input'].str.split('*', expand=True)\n",
    "\n",
    "    # Rename the new columns if needed\n",
    "    df_exploded.columns = [f'part_{i}' for i in range(1, df_exploded.shape[1] + 1)]\n",
    "\n",
    "    # Concatenate the exploded DataFrame with the original DataFrame\n",
    "    passed_3_asterisks_result = pd.concat([passed_3_asterisks, df_exploded], axis=1)\n",
    "    # Output for all the distribution logs with 3 steps - optimum\n",
    "    # passed_3_asterisks_result.rename(columns={'part_1': 'Step one', 'part_2': 'id_no', 'part_3': 'val_1', 'part_4': 'val_2'})\n",
    "    passed_3_asterisks_result.to_sql('query_audit',source_engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling all cases with 000\n",
    "def handle3Zero(payload):\n",
    "    df_distribution = payload\n",
    "    to_step_one_df = df_distribution[df_distribution['user_input'].str.contains(delimiter_000)]\n",
    "    to_step_one_df_split = to_step_one_df['user_input'].str.split(delimiter_000).str[-1]\n",
    "    # to_step_one_df_explode = to_step_one_df_split['user_input'].str.split('*', expand=True)\n",
    "    to_step_one_df_split_frame = to_step_one_df_split.to_frame(name='user_input_2')\n",
    "    to_step_one_df_split_frame\n",
    "    df_exploded_000 = to_step_one_df_split_frame['user_input_2'].str.split('*', expand=True)\n",
    "\n",
    "    # Rename the new columns if needed\n",
    "    # to_step_one_df_split.columns = [f'part_{i}' for i in range(1, df_exploded.shape[1] + 1)]\n",
    "\n",
    "    # Concatenate the exploded DataFrame with the original DataFrame\n",
    "    df_exploded_000_passed = pd.concat([to_step_one_df, df_exploded_000], axis=1)\n",
    "    df_exploded_000_passed.to_csv('output/handle3Zero.csv')\n",
    "\n",
    "    pd_000 = pd.read_excel('output/with 000 .xlsx', sheet_name='upload')\n",
    "    # pd_000.to_sql('query_audit',source_engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling all cases with 00\n",
    "def handleDoubleZero(payload):\n",
    "    one_step_back = df_distribution[df_distribution['user_input'].str.contains(delimiter_00) & ~df_distribution['user_input'].str.contains(delimiter_000) & df_distribution['user_input'].str.endswith('1*1')]\n",
    "    one_step_back_exploded = one_step_back['user_input'].str.split('*', expand=True)\n",
    "    one_step_back_exploded_merged = pd.concat([one_step_back, one_step_back_exploded], axis=1)\n",
    "    one_step_back_exploded_merged\n",
    "    one_step_back_exploded_merged.to_csv('output/onestepback no all steps back completed_.csv',index=False)\n",
    "\n",
    "    pd_00 = pd.read_excel('output/onestepback no all steps back completed.xlsx', sheet_name='upload')\n",
    "    # pd_00.to_sql('query_audit',source_engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles the distribution which starts in the mid session \n",
    " \n",
    "completed_distr_start_mid_session = distr_start_mid_session[distr_start_mid_session['user_input'].str.endswith(end_complete_esc)]\n",
    "\n",
    "completed_distr_start_mid_session_split = completed_distr_start_mid_session['user_input'].str.split(r'\\*4\\*').str[-1]\n",
    "completed_distr_start_mid_session_split_frame = completed_distr_start_mid_session_split.to_frame(name='user_input_2')\n",
    "completed_distr_start_mid_session_split_frame\n",
    "completed_distr_start_mid_session_split_frame_exploded = completed_distr_start_mid_session_split_frame['user_input_2'].str.split('*', expand=True)\n",
    "\n",
    "completed_distr_start_mid_session_split_frame_exploded_passed = pd.concat([completed_distr_start_mid_session, completed_distr_start_mid_session_split_frame_exploded], axis=1)\n",
    "# completed_distr_start_mid_session_split_frame_exploded_passed.to_csv('output/completed_distr_start_mid_session_split_frame_exploded_passed.csv')\n",
    "pd_start_middle = pd.read_excel('output/completed_distr_start_mid_session_split_frame_exploded_passed.xlsx', sheet_name='upload')\n",
    "pd_start_middle.count()\n",
    "# pd_start_middle.to_sql('query_audit',source_engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_distribution_query_data.to_csv('temp/incomplete distribution >= 9 string len.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles Incomplete Merge which start with step 4 distribution\n",
    "\n",
    "start_with_4 = incomplete_distribution_query_data[incomplete_distribution_query_data['user_input'].str.startswith(r'4*')]\n",
    "start_with_4.count()\n",
    "start_with_4_exploaded = start_with_4['user_input'].str.split('*', expand=True)\n",
    "start_with_4_merge = pd.concat([start_with_4, start_with_4_exploaded], axis=1)\n",
    "start_with_4_merge.to_csv('output/incomplete_start_with_4_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles distribution which start from a start over i.e. 000 then 4\n",
    "\n",
    "\n",
    "in_4_middle = incomplete_distribution_query_data[incomplete_distribution_query_data['user_input'].str.contains(r'\\*4\\*')]\n",
    "in_4_middle.count()\n",
    "in_4_middle_split = in_4_middle['user_input'].str.split(r'\\*4\\*').str[-1]\n",
    "in_4_middle_split_frame = in_4_middle_split.to_frame(name='user_input_2')\n",
    "in_4_middle_split_frame\n",
    "in_4_middle_split_frame_explod = in_4_middle_split_frame['user_input_2'].str.split('*', expand=True)\n",
    "\n",
    "\n",
    "in_4_middle_split_frame_explod_merge = pd.concat([in_4_middle, in_4_middle_split_frame_explod], axis=1)\n",
    "in_4_middle_split_frame_explod_merge.to_csv('output/in_4_middle_split_frame_explod_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252132"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_00 = pd.read_excel('output/in_4_middle_split_frame_explod_merge.xlsx', sheet_name='upload')\n",
    "pd_00.to_sql('incomplete_audit_2',source_engine, if_exists='replace')\n",
    "\n",
    "\n",
    "pd_00 = pd.read_excel('output/incomplete_start_with_4_merge.xlsx', sheet_name='upload')\n",
    "pd_00.to_sql('incomplete_audit_2',source_engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_audits = pd.read_sql('select * from incomplete_audit_2', source_engine)\n",
    "query_audit = pd.read_sql('select * from query_audit', source_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_audit = read_audits[~read_audits['part_2'].isin(query_audit['part_2'].astype(int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dd_hh = house_holds.merge(distribution_details, how='left', left_on='id', right_on='hh_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_ids = pd.DataFrame(read_audit['part_2'])\n",
    "unique_id_audit = read_audit['part_2'].astype(str)\n",
    "\n",
    "incomplete = merge_dd_hh[~merge_dd_hh['id_no'].isin(unique_id_audit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete['no_of_nets'].sum()\n",
    "\n",
    "unique_id_audit = read_audit['part_2'].astype(str)\n",
    "\n",
    "jane = merge_dd_hh[merge_dd_hh['id_no'].isin(unique_id_audit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_au = jane.merge(admin_units, how='left', left_on='location_id', right_on='village_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "homabay = merge_au[merge_au['county'] == 'Homa Bay County']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "homabay['actual_allocation'].sum()- homabay['no_of_nets'].sum()\n",
    "# hm = homabay[homabay['no_of_nets'].isna()]\n",
    "\n",
    "# hm['no_of_nets'].unique()\n",
    "\n",
    "homabay_not  = pd.read_csv('reports/homabay distribution.csv', dtype='str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "helo = homabay_not[homabay_not['id_no'].isin(unique_id_audit)]\n",
    "# type(homabay_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = helo['original_allocation'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_audits['part_2'].unique()\n",
    "query_audit['part_2'].unique()\n",
    "\n",
    "homabay.to_csv(f'output/homabay_incomplete{getTimestamp()}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
